---
license: gemma
language:
- fa
base_model: unsloth/gemma-3n-E4B-it
tags:
- gemma-3n
- unsloth
- persian
- farsi
- conversational
- qlora
- fine-tuned
- chat
- instruction-following
datasets:
- mshojaei77/persian-gk
model-index:
- name: gemma-3n-E4B-persian
  results: []
pipeline_tag: text-generation
widget:
- example_title: "Persian History Question"
  text: |
    <start_of_turn>user
    Ø³Ù„Ø§Ù…! Ù„Ø·ÙØ§Ù‹ Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡ÛŒ Ø¨Ø¯Ù‡ÛŒØ¯.<end_of_turn>
    <start_of_turn>model
    Ø³Ù„Ø§Ù…! ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† Ø¨Ø³ÛŒØ§Ø± ØºÙ†ÛŒ Ùˆ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø§Ø³Øª. Ø§ÛŒØ±Ø§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† ØªÙ…Ø¯Ù†â€ŒÙ‡Ø§ÛŒ Ø¬Ù‡Ø§Ù† Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¨ÛŒØ´ Ø§Ø² Û²ÛµÛ°Û° Ø³Ø§Ù„ Ù‚Ø¯Ù…Øª Ø¯Ø§Ø±Ø¯. Ø§Ø² Ø§Ù…Ù¾Ø±Ø§ØªÙˆØ±ÛŒ Ù‡Ø®Ø§Ù…Ù†Ø´ÛŒ Ú©Ù‡ ØªÙˆØ³Ø· Ú©ÙˆØ±ÙˆØ´ Ú©Ø¨ÛŒØ± Ø¨Ù†ÛŒØ§Ù† Ú¯Ø°Ø§Ø´ØªÙ‡ Ø´Ø¯ ØªØ§ Ø¯ÙˆØ±Ø§Ù† ØµÙÙˆÛŒÙ‡ØŒ Ù‚Ø§Ø¬Ø§Ø± Ùˆ Ù¾Ù‡Ù„ÙˆÛŒØŒ Ø§ÛŒØ±Ø§Ù† Ù‡Ù…ÙˆØ§Ø±Ù‡ Ù†Ù‚Ø´ Ù…Ù‡Ù…ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Ø¬Ù‡Ø§Ù† Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª.<end_of_turn>
- example_title: "Persian Culture Question"
  text: |
    <start_of_turn>user
    Ø¨Ø§Øº ØªØ®Øª Ú†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ù‡ Ú©Ù‡ Ø§ÙˆÙ† Ø±Ùˆ Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø¨Ø§Øºâ€ŒÙ‡Ø§ÛŒ Ø´ÛŒØ±Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ØŸ<end_of_turn>
    <start_of_turn>model
    Ø¨Ø§Øº ØªØ®Øª Ø¯Ø± Ø´ÛŒØ±Ø§Ø² ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø§Øºâ€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø²ÛŒØ¨Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ÛŒ Ø¯Ø§Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø§Øº Ø¨Ø§ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³Ù†ØªÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒØŒ Ø¢Ø¨â€ŒÙ†Ù…Ø§Ù‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ØŒ Ø¯Ø±Ø®ØªØ§Ù† Ú©Ù‡Ù†Ø³Ø§Ù„ Ùˆ ÙØ¶Ø§ÛŒ Ø³Ø¨Ø² Ú¯Ø³ØªØ±Ø¯Ù‡ Ø´Ù†Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø·Ø±Ø§Ø­ÛŒ Ú†Ù‡Ø§Ø±Ø¨Ø§ØºÛŒ Ø¢Ù† Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ù†Ø§ØµØ± Ø·Ø¨ÛŒØ¹ÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ø¨Ø§Øºâ€ŒØ³Ø§Ø²ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.<end_of_turn>
- example_title: "General Knowledge Question"
  text: |
    <start_of_turn>user
    Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ù¾Ø§ÛŒØªÙˆÙ† Ú†ÛŒØ³Øª Ùˆ Ú†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯ØŸ<end_of_turn>
    <start_of_turn>model
    Ù¾Ø§ÛŒØªÙˆÙ† ÛŒÚ© Ø²Ø¨Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ùˆ Ø¢Ø³Ø§Ù† Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û¹Û± ØªÙˆØ³Ø· Ú¯ÛŒØ¯Ùˆ ÙˆÙ† Ø±ÙˆØ³ÙˆÙ… Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯. Ø§ÛŒÙ† Ø²Ø¨Ø§Ù† Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ú¯Ø³ØªØ±Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø±Ø¯ Ø§Ø² Ø¬Ù…Ù„Ù‡: ØªÙˆØ³Ø¹Ù‡ ÙˆØ¨ØŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†ØŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ØŒ Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†ØŒ ØªÙˆØ³Ø¹Ù‡ Ø¨Ø§Ø²ÛŒ Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø³Ú©ØªØ§Ù¾. Ø³Ø§Ø¯Ú¯ÛŒ Ù†Ø­Ùˆ Ùˆ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØºÙ†ÛŒ Ø¢Ù†ØŒ Ù¾Ø§ÛŒØªÙˆÙ† Ø±Ø§ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÙˆÙ„ Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³Ø§Ù† ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.<end_of_turn>
---

# Gemma-3N 4B Persian - General Knowledge

<div align="center">
  <img src="https://github.com/user-attachments/assets/0c52d460-1831-46aa-b3e6-b1a5249c0174" alt="Hugging Face" width="500"/>
  <br>
  <strong>ğŸ‡®ğŸ‡· Persian Language Model | ğŸ¤– Conversational AI | ğŸ“š General Knowledge</strong>
</div>

## Model Description

This model is a fine-tuned version of `unsloth/gemma-3n-E4B-it`, optimized for Persian (Farsi) conversational tasks focused on general knowledge. It employs QLoRA techniques for efficient adaptation and is merged into a standalone model suitable for deployment.

## Model Details

### Base Model and Architecture
- **Base Model**: `unsloth/gemma-3n-E4B-it` (Google Gemma 3N 4B Instruction-Tuned).
- **Model Type**: Causal language model.
- **Model Size**: Approximately 9.9 GB (16-bit precision).
- **Context Length**: Supports up to 32,768 tokens, trained with 4,000 tokens.
- **Vocabulary**: Gemma tokenizer vocabulary.

### Intended Uses
This model is designed for direct use in Persian conversational AI, including instruction-following and general knowledge queries in domains such as Persian heritage, programming, architecture, and tourism. It is suitable for downstream applications like chat interfaces or educational tools. Out-of-scope uses include non-Persian languages or safety-critical applications.

## How to Use

### Quick Start with Transformers

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "mshojaei77/gemma-3n-E4B-persian"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

messages = [{"role": "user", "content": "Ø³Ù„Ø§Ù…! Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡ÛŒØ¯."}]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)

outputs = model.generate(
    inputs,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.95,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id,
)
response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
print(response)
```

Recommended parameters: `max_new_tokens=256-512`, `temperature=0.1-0.7`, `top_p=0.9-0.95`.

For memory optimization, use 8-bit quantization:

```python
from transformers import BitsAndBytesConfig

quant_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant_config, device_map="auto")
```

## Training

### Training Data
- **Dataset**: `mshojaei77/persian-gk` (cleaned version: `mshojaei77/persian-gk-cleaned`), comprising 5,897 Persian conversations in ChatML format.
- **Domains**: Programming, Persian heritage, architecture, tourism, and general Q&A.
- **License**: CC-BY-4.0.

### Training Procedure
The model was fine-tuned using QLoRA with 4-bit quantization.
- **LoRA Parameters**: Rank=8, alpha=16, dropout=0.0; target modules: `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`.
- **Hyperparameters**: Learning rate=2e-5, batch size=2 (effective=8 with gradient accumulation=4), epochs=1, optimizer=AdamW 8-bit, weight decay=0.01, warmup steps=10, linear LR scheduler, seed=3407.
- **Framework**: Unsloth with Weights & Biases monitoring.
- **Infrastructure**: Google Colab with GPU acceleration.

The merging process integrated LoRA adapters into the base model, converting to 16-bit precision for standalone use.

## Evaluation Results

The model achieved a final training loss of 1.78, with gradient norms stabilizing between 0.7 and 2.0. Training completed in 2 hours and 20 minutes on a T4 GPU.

Inference performance:

| Scenario | GPU | max_new_tokens=256 | Runtime |
|----------|-----|--------------------|---------|
| Single prompt | RTX T4 (16 GB) | 8.5 s | 22 tok sâ»Â¹ |
| Batch 4 | RTX T4 | 19 s | 54 tok sâ»Â¹ aggregated |

For detailed analyses of training dynamics, including loss and gradient norm charts, refer to the technical report.

## Bias, Risks, and Limitations

### Limitations

* **Language Scope**: The model is optimised for Persian (Farsi). Responses in other languages may be less fluent or factually reliable.  
* **Knowledge Cut-off**: Training data ends at January 2024; the model lacks awareness of subsequent events.  
* **Hallucination**: Like other LLMs, it can generate plausible-sounding but incorrect or fabricated information. Always verify critical outputs.  
* **Context Window**: Although the architecture supports 32 k tokens, prompts exceeding 4 k tokens were not present during training and may degrade performance.  
* **Domain Transfer**: Performance may drop on highly specialised or safety-critical domains (medical, legal, financial) that are under-represented in the dataset.  
* **Compute Requirements**: FP16 inference needs â‰ˆ 10 GB GPU VRAM; use 8-bit/4-bit quantisation for lower-resource devices.
* **Dataset Scale**: Limited to ~6k pairs, potentially overlooking linguistic diversity.
* **Training Regimen**: Single-epoch training may not fully optimize performance.

### Ethical & Safety Considerations

* The model may reflect cultural or societal biases found in the source data.  
* Do **not** rely on the model as the sole source of truth for professional advice (medical, legal, financial, etc.).  
* Implement content filtering and human oversight when deploying user-facing applications, especially for minors or vulnerable groups.  
* Comply with the Gemma Terms of Use, dataset licence (CC-BY-4.0), and local regulations on user privacy and content moderation.
* Potential for misuse in generating harmful content; mitigations include prompt engineering and output filtering.

### Environmental Impact
Training emitted approximately 0.5 kg COâ‚‚ equivalent, based on GPU usage and regional electricity factors.



## Reproduction

For detailed technical information about the training process, methodology, and evaluation results, see the [technical report](https://github.com/mshojaei77/gemma-3n-E4B-persian-qlora/blob/main/technical_report.md).

## Related Resources

- **Base Model**: `unsloth/gemma-3n-E4B-it`.
- **Adapters**: `mshojaei77/gemma-3n-E4B-persian-lora-adapters`.
- **Dataset**: `mshojaei77/persian-gk`.
- **GitHub**: [mshojaei77/gemma-3n-E4B-persian-qlora](https://github.com/mshojaei77/gemma-3n-E4B-persian-qlora).
- **Frameworks**: Unsloth (arXiv:2305.14314), PEFT (arXiv:2106.09685), Transformers.

## Citation

```bibtex
@misc{gemma3n_persian_2025,
  title={Gemma-3N 4B Persian Fine-tuned Model},
  author={Shojaei, M.},
  year={2025},
  publisher={Hugging Face},
  url={https://huggingface.co/mshojaei77/gemma-3n-E4B-persian},
  note={Fine-tuned using QLoRA on Persian General Knowledge dataset}
}
```

Dataset citation:

```bibtex
@misc{persian_gk_2025,
  title={persian-gk: Persian General Knowledge Chat Dataset},
  author={Shojaei, M. and Contributors},
  year={2025},
  url={https://huggingface.co/datasets/mshojaei77/persian-gk}
}
```

## License

Licensed under the Gemma Terms of Use (https://ai.google.dev/gemma/terms). Downstream users must adhere to these terms.

## Acknowledgments

Thanks to Google for the Gemma architecture, the Unsloth team for training tools, Hugging Face for hosting, and the Persian NLP community for contributions.